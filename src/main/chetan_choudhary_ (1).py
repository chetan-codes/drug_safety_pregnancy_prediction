# -*- coding: utf-8 -*-
"""Chetan_Choudhary_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfrS1lbRK_zyPDgSVu_z50df5m-DJ4r4
"""

!git clone https://github.com/chetan-codes/drug_safety_pregnancy_prediction.git

cd drug_safety_pregnancy_prediction/

ls

!pip install group_lasso
!pip install catboost
!pip install tpot
!pip install pydoe

"""##Main.py file"""

import random
from sklearn.pipeline import make_pipeline
import time
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import roc_auc_score, average_precision_score,make_scorer,cohen_kappa_score
from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold
import os
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, Normalizer
from src.drug_classification.supervised_dimensionality_reduction import  get_col_clusters
from group_lasso import LogisticGroupLasso
from src.data_readers.tagged_preg_reader import tagged_data_reader
from src.drug_classification.supervised_dimensionality_reduction import extract_text_features
from src.drug_classification.pregnancy_drug_experment import  transform_labels, preg_scorer, senstivity, specficity, f1_threshold, precision_threshold,recall_threshold
from src.multimodal_learning.multimodal_classifiers import multimodal_classifiers
import pandas as pd

#Installing XGBoost and LightGBM for testing
!pip install XGBoost
!pip install LightGBM

#Importing my base models
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam

"""##Exploratory Data Analysis"""

print(X_unlabled.shape)

X_unlabled = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/drugData.csv.gz',compression='gzip',index_col=0)
X_unlabled.head()

modalities_df = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/modalities.csv.gz',compression='gzip',index_col=0)
print(modalities_df.shape)
modalities_df.head()

#List of Drug Names
drug_names_df = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/drug_names.csv.gz',compression='gzip',index_col=0)
print(drug_names_df.shape)
drug_names_df.head()

#Modalities with text data as features.
X_unlabeled_mod_w_text_df = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/modalities_w_text.csv.gz',compression='gzip',index_col=0)
X_unlabeled_mod_w_text_df.head()

#Drug Data from DrugBank that is cleaned and combined with the text data with comments.
X_unlabeled_w_text_df = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/drugData_w_text.csv.gz',compression='gzip',index_col=0)

X_unlabeled_w_text_df.head()

#the function that was used to add new modality to data.
def add_mods(X, X2, mods, modality_name):
    new_mods = pd.DataFrame({'modality': [modality_name] * len(X2.columns), 'feature': X2.columns})
    mods = pd.concat([mods, new_mods], ignore_index=True)
    X = X.join(X2, how='left')
    return X, mods

#Setting seed and lo_lossees as True for group Lasso
seed= 0
random.seed(seed)
LogisticGroupLasso.LOG_LOSSES = True
#Setting dataframes to display max columns
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

#Drug were read from drugbank
X_unlabled = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/drugData.csv.gz',compression='gzip',index_col=0)
modalities_df = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/modalities.csv.gz',compression='gzip',index_col=0)
print('done reading from disk')

#Setting relevant modalities
base_mods = ['ATC_Level_1_description', 'ATC_Level_2_description', 'ATC_Level_3_description', 'ATC_Level_4_description',  'ATC_Level_5',  'Associated_condition',  'Carrier','Category',  'Enzyme', 'Group',  'Target', 'Taxonomy', 'Transporter', 'Type']
mods_domain_expert = ['Category', 'ATC_Level_3_description', 'ATC_Level_2_description', 'ATC_Level_4_description', 'Associated_condition', 'Taxonomy']

#Adding our features
X_unlabled_text = extract_text_features(X_unlabled[modalities_df.loc[modalities_df.modality.isin(mods_domain_expert), 'feature']])
#New Line
#X_unlabled_text = extract_text_features(X_unlabled[modalities_df.loc[modalities_df.modality.isin(mods_domain_expert), 'feature'].tolist()])
X_unlabled, modalities_df= add_mods(X_unlabled, X_unlabled_text, modalities_df, 'Text')
print(X_unlabled_text.columns)
text_mods = ['Text']
print('done processing text')
num_clusters=3600 #Number of custers to create. 3600 was optimal in paper.
clustering_mods=[]
#X_cluster = get_col_clusters(X_unlabled[modalities_df.loc[modalities_df.modality.isin(mods_domain_expert), 'feature']], num_clusters)
#print('clusters head:')
#print(X_cluster.head())
#print(X_cluster.columns)
mod_name='Clusters'
#clustering_mods.append(mod_name)
#X_unlabled , modalities_df= add_mods(X_unlabled, X_cluster, modalities_df, mod_name)
print('done clustering')

#Writing new data to disk
#X_unlabled.to_csv(r'data\drugData_w_text.csv.gz',compression='gzip')
#modalities_df.to_csv(r'data\modalities_w_text.csv.gz',compression='gzip')

X_unlabled = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/drugData_w_text.csv.gz',compression='gzip',index_col=0)
modalities_df = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/modalities_w_text.csv.gz',compression='gzip',index_col=0)
#print("done preproc")
all_modalities = list(modalities_df.modality.unique())
print('mods:',all_modalities)

"""##Setting and defining the Cross Validation and Cross Validation with expert Evaluation functions"""

def cv_eval(X_unlabled, classifiers, exp_name):
    # ###### CV exp
    random.seed(seed)
    kf = RepeatedStratifiedKFold(n_splits=2, random_state=0, n_repeats=reps)
    preg_tagged_data_reader = tagged_data_reader()
    X_cv = X_unlabled.join(
        preg_tagged_data_reader.read_all(remove_disputed=True, read_smc=True, read_Eltonsy_et_al=False,
                                         read_safeFetus=False), how='inner')
    y_cv = X_cv['preg_class']
    y_cv = pd.Series(transform_labels(y_cv), index=y_cv.index, name='tag')
    del X_cv['preg_class']

    results_collector = preg_scorer()
    start_time = time.monotonic()
    for current_mods,current_algorithm,clf in classifiers:
        cv_score_all = cross_validate(estimator=clf, X=X_cv, y=y_cv, cv=kf, scoring=metrics, n_jobs=1)
        results_collector.add_from_cv(current_mods, current_algorithm,  cv_score_all)
    print('minutes: ',(time.monotonic() - start_time)/60)
    print(results_collector.as_df().groupby(['Name','Model','Modalities']).mean().sort_values(by='AUC',))
    results_collector.as_df().to_csv(os.path.join('output','results_'+exp_name+'_cv.csv'))


def cross_expert_eval(X_unlabled, classifiers, exp_name,remove_disputed):
    preg_tagged_data_reader = tagged_data_reader()
    train_set = X_unlabled.join(preg_tagged_data_reader.read_all(remove_disputed=remove_disputed,read_who=True,
                                                                 read_smc=False,read_Eltonsy_et_al=False,read_safeFetus=False),how='inner')
    y_train = train_set['preg_class']
    y_train = pd.Series(transform_labels(y_train),index=y_train.index,name='tag')
    X_train = train_set
    del X_train['preg_class']

    test_set = X_unlabled.join(preg_tagged_data_reader.read_all(remove_disputed=remove_disputed,read_who=False,
                                                                read_smc=True,read_Eltonsy_et_al=False,
                                                                read_safeFetus=False),how='inner')
    print('Removing drug appearing in train ans test both:',len(set(test_set.index) & set(X_train.index)))

    test_set = test_set.loc[~test_set.index.isin(train_set.index)]
    y_test = test_set['preg_class']
    y_test = pd.Series(transform_labels(y_test),index=y_test.index,name='tag')
    X_test = test_set
    del X_test['preg_class']

    results_collector_db = preg_scorer()

    for current_mods,current_algorithm,clf in classifiers:
        for rep in range(reps):
            try:
                clf.fit(X_train, y_train)
                ExtraTrees_preds = clf.predict_proba(X_test)[:, 1]
                results_collector_db.add_score(current_mods,current_algorithm, y_test, ExtraTrees_preds, rep, 0, 0)
            except:
                print('cannot run',current_algorithm)
    print(results_collector_db.as_df().groupby(['Name','Model','Modalities']).mean().sort_values(by='AUC'))
    results_collector_db.as_df().to_csv(os.path.join('output','results_'+exp_name+'_db.csv'))
    print('Number of drugs in train but not in test:',len(set(X_train.index) - set(X_test.index)))
    print('Number of drugs in test but not in train:',len(set(X_test.index) - set(X_train.index)))
    print('Number of drugs in test but not in both:',len(set(X_test.index) & set(X_train.index)))

metrics = {'cohen_kappa_score':make_scorer(cohen_kappa_score),'roc_auc':make_scorer(roc_auc_score,needs_proba=True), 'f1':make_scorer(f1_threshold,needs_proba=True), 'precision':make_scorer(precision_threshold,needs_proba=True), 'recall':make_scorer(recall_threshold,needs_proba=True),
           'average_precision':make_scorer(average_precision_score,needs_proba=True),'sensitivity':make_scorer(senstivity,needs_proba=True),'specificity':make_scorer(specficity,needs_proba=True)}
reps=1 #number of CV and cross db repeats. 30 was used in paper

classifiers_man = multimodal_classifiers(modalities_df,list(X_unlabled.columns))
from sklearn.neural_network import MLPClassifier

# Define the suggested parameter values
hidden_layer_sizes = (200,100)  # Example: (100,) or (50,) or (200, 100)
activation = 'relu'  # Example: 'logistic', 'tanh', 'identity'
solver = 'adam'  # Example: 'sgd', 'adam', 'lbfgs'
alpha = 0.0001  # Example: 0.0001 - 0.1
learning_rate_init = 0.001  # Example: 0.001 - 0.01
max_iter = 1000  # Example: 200 - 1000
batch_size = 'auto'  # Example: 16, 32, 64
momentum = 0.9  # Example: 0.9
learning_rate_schedule = 'adaptive'  # Example: 'adaptive'

# Create MLPClassifier with the suggested parameter values
mlp_classifier = MLPClassifier(
    hidden_layer_sizes=hidden_layer_sizes,
    activation=activation,
    solver=solver,
    alpha=alpha,
    learning_rate_init=learning_rate_init,
    max_iter=max_iter,
    batch_size=batch_size,
    momentum=momentum,
    learning_rate=learning_rate_schedule
)



#Base models we use
algos = [
        ('ExtraTreesClassifier',ExtraTreesClassifier(n_jobs=1)),
        ('MLPClassifier', mlp_classifier),
        ('XGBClassifier', XGBClassifier(n_jobs=1)),  # XGBoost
    ('LGBMClassifier', LGBMClassifier(n_jobs=1)),  # LightGBM
         ]

#Adding all modality selection and base model combinations
classifiers_exp_single_mods = []
for algo_name, algo in algos: #all mods
    classifiers_exp_single_mods.append((['Lasso'], algo_name, classifiers_man.get_lasso_classifier(base_mods, algo, 0.01, X_unlabled[ modalities_df.loc[modalities_df.modality.isin(base_mods), 'feature']])))


# large ortho, it uses the same base models, makes no sense to put it in the loop.
for mods_name, m in [
    ('All', base_mods),
    ('Domain expert', mods_domain_expert),
    ('All + Clusters', base_mods + clustering_mods),
    ('All + Text', base_mods + text_mods),
                     ]:
    voting_clf_ort_large, stacking_clf_ort_large = classifiers_man.get_orthogonal_large_voting_stacking(m)
    classifiers_exp_single_mods.append(([mods_name],'Voting orthogonal large',voting_clf_ort_large))

#Adding all single modalities with base models
for algo_name, algo in algos: #all algo
    for m in [[x] for x in base_mods]:
        print(m)
        classifiers_exp_single_mods.append((m, algo_name, classifiers_man.get_modalities_base_clf(m, algo)))

import pandas as pd
import matplotlib.pyplot as plt

# Read cross-validation data
cv_data = pd.read_csv("/content/results_all_exp_cv (2).csv")

# Read cross-expert validation data
cev_data = pd.read_csv("/content/results_all_exp_db.csv")

# Function to create bar plots
def plot_sensitivity_comparison(data, title):
    # Sort data by sensitivity in descending order
    sorted_data = data.sort_values(by='Sensitivity', ascending=False).head(5)

    plt.figure(figsize=(10, 6))
    plt.bar(sorted_data['Model'], sorted_data['Sensitivity'], color='skyblue')
    plt.xlabel('Model')
    plt.ylabel('Sensitivity')
    plt.title(title)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Create bar plots for cross-validation and cross-expert validation
plot_sensitivity_comparison(cv_data, 'Sensitivity Comparison - Cross-Validation')
plot_sensitivity_comparison(cev_data, 'Sensitivity Comparison - Cross-Expert Validation')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Read cross-validation data
cv_data = pd.read_csv("/content/results_all_exp_cv (2).csv")

# Read cross-expert validation data
cev_data = pd.read_csv("/content/results_all_exp_db.csv")

import pandas as pd
import matplotlib.pyplot as plt



# Function to create bar plots
def plot_auc_comparison(data, title):
    plt.figure(figsize=(10, 6))
    plt.bar(data['Model'], data['AUC'], color='skyblue')
    plt.xlabel('Model')
    plt.ylabel('AUC Score')
    plt.title(title)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Create bar plots for cross-validation and cross-expert validation
plot_auc_comparison(cv_data, 'AUC Scores - Cross-Validation')
plot_auc_comparison(cev_data, 'AUC Scores - Cross-Expert Validation')

#Run experiments
results_collector_nn_iteration_3 = cv_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp')
results_collector_db = cross_expert_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp',remove_disputed=True)
#results_collector_db_w_disagreed = cross_expert_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp_w_disputed',remove_disputed=False

results_collector_db_w_disagreed = cross_expert_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp_w_disputed',remove_disputed=False)

#Run experiments with hidden_layer = (100,)
results_collector_nn_iteration_1 = cv_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp')

#Run experiments
results_collector_nn = cv_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp')

results_collector_nn.dtype

"""##LightGBM Dry Run"""

#Run experiments
results_collector_lgbm = cv_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp')
#results_collector_db = cross_expert_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp',remove_disputed=True)
#results_collector_db_w_disagreed = cross_expert_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp_w_disputed',remove_disputed=False)



"""##Results for XGBoost Dry Run"""

#Run experiments
results_collector = cv_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp')
#results_collector_db = cross_expert_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp',remove_disputed=True)
#results_collector_db_w_disagreed = cross_expert_eval(X_unlabled, classifiers_exp_single_mods, exp_name='all_exp_w_disputed',remove_disputed=False)



"""##Supervised Dimensionality reduction PY

"""

import operator
import os
from collections import Counter

import pandas as pd
import keras
from keras import layers, regularizers
from keras.layers import multiply, Multiply, Concatenate
from pandas import HDFStore
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import pairwise_distances, roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np



X_unlabled = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/sample_data_s.csv.gz',compression='gzip',index_col=0)

def getPCA(X_unlabled,features,modality,k=100):
    # import prince
    # pca = prince.MCA(
    #     n_components=1000,
    #  #n_iter=3,
    #  copy=True,
    #  check_input=True,
    #  engine='auto',
    #  random_state=42 )
    # X_unlabled_pca = pca.fit_transform(
    #     VarianceThreshold().fit_transform(
    #         X_unlabled[modalities.loc[modalities.modality.isin([modality]), 'feature']]
    #     )
    # )

    #pca = corex.Corex(n_hidden=50, dim_hidden=10, marginal_description='discrete', smooth_marginals=False,max_iter=100,n_repeat=1,n_cpu=None,verbose=True) # n_hidden = dim of output. dim_hiden=card of output#too slow
    # Define the number of hidden factors to use (n_hidden=2).
    # And each latent factor is binary (dim_hidden=2)
    # marginal_description can be 'discrete' or 'gaussian' if your data is continuous
    # smooth_marginals = True turns on Bayesian smoothing
    #layer1.fit(X)  # Fit on data.
    k = round(len(features) ** (0.5) )
    pca = PCA(n_components=k)
    X_unlabled_pca = pca.fit_transform(
       StandardScaler().fit_transform(X_unlabled[features]))

    X_unlabled_pca = pd.DataFrame(X_unlabled_pca)
    X_unlabled_pca.index = X_unlabled.index
    X_unlabled_pca.columns = [str(modality)+'_PCA' +"_" + str(x) for x in range(pca.n_components)]
    return X_unlabled_pca

!pip install fasttext

import fasttext

def getselectK(X_unlabled, modality,t=0.01):
    select = VarianceThreshold(t)
    data = X_unlabled[modalities.loc[modalities.modality.isin([modality]), 'feature']]
    select.fit(data)
    data = data.iloc[:,select.get_support()]
    data.columns = [x+ ' select' for x in data.columns]
    return data

def extract_text_features(X_unlabled):
    output = X_unlabled.copy()
    output.columns = [c.split(': ')[1] if ': ' in c else c for c in output.columns]
    a = pd.melt(output.reset_index(), id_vars=['drugBank_id'])
    a.value = a.value.astype(bool)
    a = a[a.value == True]
    a = a.groupby('drugBank_id').variable.apply(lambda x: "%s " % ' '.join(x))

    #count_vect = CountVectorizer(binary=True)
    a = a.fillna('')
    fasttext_model = fasttext.train_unsupervised(a, model='skipgram', dim=100)

    #New Code
    #Using FastText Word Embedding Instead of BagofWords using countVectorizer.
    word_embeddings = fasttext_model.fit_transform(a)
    text_features = ['Embedding_' + str(i) for i in range(word_embeddings.shape[1])]
    text_features_df = pd.DataFrame(word_embeddings, columns=text_features, index=a.index)
    a = pd.DataFrame(index=X_unlabled.index).join(text_features_df, how='left').fillna('')

    #word_counts = count_vect.fit_transform(a)
    #text_features = ['Mention: ' + x for x in count_vect.get_feature_names()]
    #text_features = pd.DataFrame(word_counts.toarray(), columns=text_features, index=a.index)
    #a = pd.DataFrame(index=X_unlabled.index).join(text_features, how='left').fillna('')
    #Below is old commented code
    # for c in text_features:
    #     modalities = modalities.append({'modality': 'text_processed', 'feature': str(c)}, ignore_index=True)
    return a#, modalities

def get_col_clusters(X_unlabled,number_of_clusters):
    #X_unlabled.columns = [x.replace(' description','').lower().replace('atc level ','ATC Level ') for x in X_unlabled.columns] #remove the description of the ATC codes
    s = pd.Series(X_unlabled.columns, index=X_unlabled.columns)
    s = s[~s.str.contains('Number of')]
    newvals = [c.split(': ')[1] if ': ' in c else c for c in s.values]
    s = pd.Series(newvals, index=s.index)
    count_vect = CountVectorizer(binary=True) #2-5 char_wb improves cat
    #count_vect = TfidfVectorizer(ngram_range=(2, 5), analyzer='char_wb',max_features=1000)  # 2-5 char_wb improves cat
    word_counts = count_vect.fit_transform(s)
    word_counts = word_counts
    text_features = count_vect.get_feature_names()
    text_features = pd.DataFrame(word_counts.toarray(), columns=text_features, index=s.index)
    ans = None
    print('features,words:',text_features.values.shape)
    print(list(text_features.columns)[:1000])
    dist_mat = pairwise_distances(text_features.values, metric='jaccard', n_jobs=1) #jaccard
    print('done sim matrix')
    #single; 9000: 0.8798, 7500:0.877
    #average selected mods: 2500 not good. 4000:0.882\0.896. 3500 0.88\0.893
    #average: 3500: best. 3750: small reduction. answer is in between
    kmeans = AgglomerativeClustering(n_clusters=number_of_clusters,linkage='average',affinity='precomputed')#affinity=sklearn.metrics.jaccard_score ,affinity='cosine'
    #kmeans.fit(text_features)
    clusters = pd.DataFrame(kmeans.fit_predict(dist_mat),columns=['cluster'],index=text_features.index)
    for g in clusters.groupby('cluster').groups:
        g= clusters.groupby('cluster').groups[g]
        col_name='Cluster '+str(number_of_clusters)+ ' :  ' + '; '.join([str(gr) for gr in g])
        g_col = X_unlabled[g].sum(axis=1).astype(bool)
        if ans is None:
            g_col.name = col_name
            ans = pd.DataFrame(g_col)
        else:
            ans[col_name] = g_col
    print('done clustering',number_of_clusters)

    print('done clustering')
    return ans


def print_topics(model, count_vectorizer, n_top_words):
    words = count_vectorizer.get_feature_names()
    c = Counter()
    for topic_idx, topic in enumerate(model.components_):
        print("\nTopic #%d:" % topic_idx)
        print(" ".join([words[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
        print(topic.argsort()[:-n_top_words - 1:-1])
        c.update(topic.argsort()[:-n_top_words - 1:1])
    print(c.most_common())

def get_lda_cluster(X_unlabled,num_clusters):
    #X_unlabled.columns = [x.replace(' description','').lower().replace('atc level ','ATC Level ') for x in X_unlabled.columns] #remove the description of the ATC codes
    columns_data = pd.Series(X_unlabled.columns, index=X_unlabled.columns)
    columns_data = columns_data[~columns_data.str.contains('Number of')]
    newvals = [c.split(': ')[1] if ': ' in c else c for c in columns_data.values]
    columns_data = pd.Series(newvals, index=columns_data.index)
     # Create a corpus from a list of texts

    count_vect = CountVectorizer(stop_words='english')
    word_counts = count_vect.fit_transform(columns_data)
    #text_features = count_vect.get_feature_names()
    #text_features = pd.DataFrame(word_counts.toarray(), columns=text_features, index=columns_data.index)
    # Create and fit the LDA model
    from sklearn.decomposition import LatentDirichletAllocation as LDA
    #import lda
    #lda = lda.LDA(n_topics=num_clusters,random_state=0,n_iter=2000,eta=0.001,alpha =0.01) #mean_change_tol=1e-8,max_iter=100, ,learning_method='online',batch_size=2500
    lda = LDA(n_components=num_clusters, random_state=0,n_jobs=1,learning_method='online',verbose=1,max_iter=50,evaluate_every=5)  # mean_change_tol=1e-8,max_iter=100, ,

    print(word_counts.shape)

    predicted_cluster = lda.fit_transform(word_counts).argmax(axis=1)
    #print_topics(lda, count_vect, 5)

    print(predicted_cluster.shape)
    predicted_cluster = pd.Series(predicted_cluster, index=columns_data.index)

    # predicted_cluster = lda.fit_transform(word_counts).argsort(axis=1)[:,:num_cluster_members]
    # print_topics(lda,count_vect,5)
    # print(predicted_cluster.shape)
    # predicted_cluster = pd.DataFrame(predicted_cluster, index=columns_data.index,columns=[str(x) for x in range(num_cluster_members)])
    # predicted_cluster = pd.concat([predicted_cluster[str(x)] for x in range(num_cluster_members)])
    # print(predicted_cluster.shape)
    ans=None
    print('predicted num clusters',len(predicted_cluster.unique()))
    for curr_cluster in range(num_clusters):
        curr_cols = predicted_cluster[predicted_cluster==curr_cluster]
        if len(curr_cols) >0:
            #print('curr cluster size',len(curr_cols))
            col_name = 'Cluster ('+str(num_clusters)+') '+str(curr_cluster)+': ' + '; '.join([str(col) for col in curr_cols.index])
            #print(col_name)
            g_col = X_unlabled[curr_cols.index].sum(axis=1).astype(bool)
            if ans is None:
                g_col.name = col_name
                ans = pd.DataFrame(g_col)
            else:
                ans[col_name] = g_col
    print('final number of clusters',ans.shape)

    print('done clustering')
    return ans

def convert_to_one_hot(X_unlabled, modalities, modality):
    cols = modalities.loc[modalities.modality.isin([modality]), 'feature']
    X_unlabled =  pd.get_dummies(X_unlabled,columns=cols,prefix=cols,prefix_sep=': ')
    modalities=modalities[modalities.modality!=modality]
    for c in X_unlabled.columns:
        if c.split(': ')[0] in cols.values:
            modalities = modalities.append({'modality': modality, 'feature': c}, ignore_index=True)
    return X_unlabled,modalities

def supervised_dim_reduction(X_train, y_train, X_test=None, validation=0.0):
    if X_test is None:
        X_test =X_train
    inputs = keras.Input(shape=(len(X_train.columns),))
    reg = None#regularizers.l1_l2(l1=1e-5, l2=1e-5)
    dense_out = layers.Dense(len(y_train.columns), activation="sigmoid", kernel_regularizer=reg)#for some weird reason softmax works better

    # mult = Multiply()([inputs, inputs])
    # layer = Concatenate()([mult,inputs])
    layer= inputs
    layers_list = []
    for i in range(10):
        layer = layers.Dropout(0.0)(layer)
        layer = layers.Dense(300, activation="relu",kernel_regularizer=reg)(layer)
        layers_list.append(layer)

    outputs = dense_out(layer)
    model = keras.Model(inputs=inputs, outputs=outputs, name="dim_reduction")
    model.compile(
        loss='binary_crossentropy',
        optimizer='Adam',
        metrics=["accuracy"],
    )
    model.fit(X_train, y_train.astype(int), shuffle=True, validation_split=validation, epochs=25, verbose=2)#epochs=25

    model_out = keras.Model(inputs=inputs, outputs=layers_list[0], name="dim_reduction")
    model_out.compile(
        loss='binary_crossentropy',
        optimizer=keras.optimizers.Adam(),
        metrics=["accuracy"],
    )
    ans = model_out.predict(X_test)
    ans = pd.DataFrame(ans)
    return ans#,model.predict(X_test)

from keras.layers import LSTM

def supervised_dim_reduction(X_train, y_train, X_test=None, validation=0.0):
    if X_test is None:
        X_test = X_train

    inputs = keras.Input(shape=(X_train.shape[1], X_train.shape[2]))
    reg = None  # You can adjust regularization as needed

    # LSTM layers
    lstm_out = LSTM(units=300, activation='relu', kernel_regularizer=reg, return_sequences=True)(inputs)
    for _ in range(9):  # Add additional LSTM layers if needed
        lstm_out = LSTM(units=300, activation='relu', kernel_regularizer=reg, return_sequences=True)(lstm_out)

    # Final LSTM layer
    lstm_out = LSTM(units=300, activation='relu', kernel_regularizer=reg)(lstm_out)

    # Output layer
    outputs = layers.Dense(len(y_train.columns), activation='sigmoid')(lstm_out)

    # Define the model
    model = keras.Model(inputs=inputs, outputs=outputs, name="dim_reduction")

    # Compile the model
    model.compile(
        loss='binary_crossentropy',
        optimizer='Adam',
        metrics=["accuracy"],
    )

    # Train the model
    model.fit(X_train, y_train.astype(int), shuffle=True, validation_split=validation, epochs=25, verbose=2)

    # Output intermediate representation
    model_out = keras.Model(inputs=inputs, outputs=lstm_out, name="dim_reduction")
    model_out.compile(
        loss='binary_crossentropy',
        optimizer=keras.optimizers.Adam(),
        metrics=["accuracy"],
    )
    ans = model_out.predict(X_test)
    ans = pd.DataFrame(ans)

    return ans

X_unlabled.head()

modalities.head()

if __name__ =='__main__':
    print('Enter Check')
    #os.chdir('..\\..')
    #store = HDFStore('output\data\modalities_dict.h5')
    #X_unlabled = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/drugData.csv.gz',compression='gzip',index_col=0)
    modalities = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/modalities_w_text.csv.gz',compression='gzip',index_col=0)
   #cols_x = modalities[modalities.modality.isin(['mol2vec'])].feature
    #cols_y = modalities[modalities.modality.isin(['Category'])].feature
    #X = X_unlabled[cols_x]
    print(modalities)
    #y = X_unlabled[cols_y].drop('Number of Category', axis=1)
    #print(y)
    # X_train, X_test, y_train, y_test = train_test_split(
    #     X, y, test_size=0.33, random_state=42)
    #ans = supervised_dim_reduction(X,y,validation=0.2)
    # 1 - y.sum().sum() / y.count().sum()

    # print(np.sum(preds.round() == y_test, axis=0).sum() / y_test.count().sum())

# Commented out IPython magic to ensure Python compatibility.
# %run src/main/main.py

X_unlabeled = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/modalities_w_text.csv.gz',compression='gzip',index_col=0)

len(X_unlabeled.columns)

X_unlabeled.shape[0]

import pandas as pd

# Assuming df is your original DataFrame

# Calculate the desired number of rows for the new DataFrame (1/4 of the original size)
desired_rows = len(X_unlabeled) // 100

# Sample the DataFrame to reduce the number of rows
sampled_data = X_unlabeled.sample(n=desired_rows)

# Save the sampled DataFrame to a CSV file with gzip compression
sampled_data.to_csv('/content/drug_safety_pregnancy_prediction/data/sample_data_w_text.csv.gz', compression='gzip', index=False)

# Print confirmation message
print("Sampled DataFrame saved to CSV with gzip compression.")

sampled_data = pd.read_csv(r'/content/drug_safety_pregnancy_prediction/data/sample_data_w_text.csv.gz',compression='gzip',index_col=0)